# ğŸ–¼ï¸ Image Captioning with Mask R-CNN + Transformer

This project implements an end-to-end **image captioning system** using:
- **Mask R-CNN** for object detection & region feature extraction  
- **ResNet backbone** for feature encoding  
- **Transformer decoder** for caption generation  
- Trained & tested on **Flickr30k dataset**

---

## âš™ï¸ Setup

### 1. Clone the repository
```bash
git clone <your-repo-url>
cd <repo-name>


project/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ extract_features.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ infer.py
â”‚   â””â”€â”€ utils/...
â”‚
â”‚â”€â”€ outputs/                # Features, checkpoints, results
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
â”‚â”€â”€ .gitignore
â”‚â”€â”€ venv/                   # (not pushed to git)


# Extract features
python extract_features.py --split train --max-images 200 --max-regions 36 --out outputs/features.npz

# Train model
python train.py --features outputs/features.npz --out outputs/checkpoints --epochs 5

# Caption an image
python infer.py --checkpoint outputs/checkpoints/model_epoch5.pth --checkpoint-dir outputs/checkpoints --image example.jpg
